{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# El conjunto de datos incluye estas imágenes\n",
        "\n",
        "`<userid> <pose> <expression> <eyes> <scale>.pgm`  \n",
        "`<userid>` es la identificación del usuario de la persona en la imagen, y este campo tiene 20 valores: an2i, at33, boland, bpm, ch4f, cheyer, choon, danieln, glickman, karyadi, kawamura, kk49, megak, mitchell, night, phoebe, saavik, steffi, sz24 y tammo.  \n",
        "`<pose>` es la posición de la cabeza de la persona, y este campo tiene 4 valores: straight (recto), left (izquierda), right (derecha), up (arriba).  \n",
        "`<expression>` es la expresión facial de la persona, y este campo tiene 4 valores: neutral, happy (feliz), sad (triste), angry (enojado).  \n",
        "`<eyes>` es el estado de los ojos de la persona, y este campo tiene 2 valores: open (abiertos), sunglasses (gafas de sol).  \n",
        "`<scale>` es la escala de la imagen, y este campo tiene 3 valores: 1, 2 y 4.   \n",
        "1 indica una imagen de resolución completa (128 columnas por 120 filas); 2 indica una imagen de resolución media (64 por 60); 4 indica una imagen de resolución cuarto (32 por 30).  \n",
        "\n",
        "Si has estado observando de cerca en los directorios de imágenes, podrías notar que algunas imágenes tienen el sufijo .bad en lugar del sufijo .pgm. Resulta que 16 de las 640 imágenes tomadas tienen fallas debido a problemas con la configuración de la cámara; estas son las imágenes .bad. Algunas personas tuvieron más problemas que otras, pero todos los que fueron \"faced\" deberían tener al menos 28 buenas imágenes faciales (de las 32 variaciones posibles, excluyendo la escala).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_-p86T2n2KH"
      },
      "source": [
        " **Nombre de los integrantes:**\n",
        "\n",
        "\n",
        "* **Integrante 1:** Juan José Monsalve Patiño \\\\\n",
        "* **Integrante 2:** Pamela Escobar Palacios \\\\\n",
        "* **Integrante 3:** José Julián Aguirre Ramírez \\\\\n",
        "* **Integrante 4:** Santiago Mejia Carmona\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhVrFqatoHvv"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://blogs.elespectador.com/wp-content/uploads/2017/09/logo-Universidad-Nacional.png\" width=\"500\" alt=\"logo\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyauN4RUoSZ9"
      },
      "source": [
        "**Definición del problema**\n",
        "\n",
        "En el contexto del conjunto de datos CMU Face Images Data Set del UCI Machine Learning Repository, se presenta el desafío de construir y validar un modelo de aprendizaje estadístico capaz de clasificar imágenes de sujetos según la presencia o ausencia de gafas.\n",
        "\n",
        "El conjunto de datos consta de 640 imágenes que representan a diversos sujetos en diferentes posiciones y expresiones faciales. Cada imagen está etiquetada con información detallada, incluyendo la identificación del usuario, la posición de la cabeza, la expresión facial, el estado de los ojos (gafas o no) y la escala de la imagen.\n",
        "\n",
        "**Objetivo**\n",
        "\n",
        "El objetivo principal es desarrollar un modelo predictivo que pueda analizar las imágenes y predecir si el sujeto lleva gafas o no. Esto implica la clasificación binaria de las imágenes en dos categorías: \"con gafas\" y \"sin gafas\". Se espera que el modelo pueda generalizar correctamente a nuevas imágenes no vistas durante el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1Na9Knooj6"
      },
      "source": [
        "## Instalar librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401HEUBwonZy",
        "outputId": "9b23864a-befe-476f-f7ad-dce7fd4cb30e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "from keras.utils import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import os\n",
        "import seaborn as sns\n",
        "from skimage import feature, io\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from skimage.feature import hog\n",
        "import random\n",
        "import itertools\n",
        "from shutil import copyfile, rmtree\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWgd6xcIpkcR"
      },
      "source": [
        "# Análisis descriptivo y exploratorio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAa-pOl9pt57",
        "outputId": "0658c916-33a8-47e8-d975-c2256df8e97a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se cuentan con 0 sin gafas\n",
            "--------------------------------------------------------------\n",
            "Se cuentan con 0 con gafas\n"
          ]
        }
      ],
      "source": [
        "sin_gafas = '/content/drive/MyDrive/imagenes_analitica/con_gafas'\n",
        "con_gafas = '/content/drive/MyDrive/imagenes_analitica/sin_gafas'\n",
        "print('Se cuentan con', len(os.listdir(sin_gafas)), 'sin gafas')\n",
        "print('--------------------------------------------------------------')\n",
        "print('Se cuentan con', len(os.listdir(con_gafas)), 'con gafas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH-LUCW8ww7n"
      },
      "source": [
        "#Cargar y Preprocesar Imágenes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DEzJcv27xrj3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from skimage import io, color, transform\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directorio que contiene las carpetas con gafas y sin gafas\n",
        "dataset_dir = '/content/drive/MyDrive/imagenes_analitica'\n",
        "\n",
        "# Cargar imágenes y etiquetas\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for label, category in enumerate(['con_gafas', 'sin_gafas']):\n",
        "    category_dir = os.path.join(dataset_dir, category)\n",
        "    for filename in os.listdir(category_dir):\n",
        "        img_path = os.path.join(category_dir, filename)\n",
        "        img = io.imread(img_path)\n",
        "        img = transform.resize(img, (64, 64, 3)).flatten()  # Ajusta el tamaño según sea necesario\n",
        "        X.append(img)\n",
        "        y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrITtvla7EnC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDeNFnPH0Y2m"
      },
      "source": [
        " # Construir y Entrenar el Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFHAXRwX0dR1",
        "outputId": "dad5d2a0-4517-45ca-e66b-db98212f63c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 17s 1s/step - loss: 11.6016 - accuracy: 0.4689 - val_loss: 6.1543 - val_accuracy: 0.4320\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 16s 1s/step - loss: 2.1524 - accuracy: 0.5651 - val_loss: 1.5810 - val_accuracy: 0.5760\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 16s 1s/step - loss: 1.1804 - accuracy: 0.5972 - val_loss: 0.5913 - val_accuracy: 0.7280\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.7365 - accuracy: 0.6593 - val_loss: 0.5155 - val_accuracy: 0.7920\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 15s 979ms/step - loss: 0.5913 - accuracy: 0.7154 - val_loss: 0.4852 - val_accuracy: 0.7760\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 14s 897ms/step - loss: 0.6079 - accuracy: 0.6954 - val_loss: 0.6030 - val_accuracy: 0.6720\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 15s 888ms/step - loss: 0.6152 - accuracy: 0.6834 - val_loss: 0.5899 - val_accuracy: 0.7280\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 16s 952ms/step - loss: 0.5764 - accuracy: 0.7295 - val_loss: 0.4718 - val_accuracy: 0.8080\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 16s 998ms/step - loss: 0.4562 - accuracy: 0.7976 - val_loss: 0.4686 - val_accuracy: 0.8160\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 16s 1s/step - loss: 0.4468 - accuracy: 0.8016 - val_loss: 0.4308 - val_accuracy: 0.8240\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cbe09af1120>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# las imágenes son en escala de grises y aplanadas (64x64x3)\n",
        "model = models.Sequential([\n",
        "    layers.Dense(4096, activation='relu', input_shape=(64*64*3,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HjFS24W3QQK"
      },
      "source": [
        "# Pruebas del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wA4Qvxp3TI4",
        "outputId": "4a1a1d08-562b-483d-c5f3-171a5d9c9f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 90ms/step\n",
            "Accuracy: 0.824\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.84        71\n",
            "           1       0.78      0.83      0.80        54\n",
            "\n",
            "    accuracy                           0.82       125\n",
            "   macro avg       0.82      0.83      0.82       125\n",
            "weighted avg       0.83      0.82      0.82       125\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Hacer predicciones en el conjunto de prueba\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Convertir las predicciones a clases binarias (0 o 1) según un umbral (por ejemplo, 0.5)\n",
        "threshold = 0.5\n",
        "predicted_classes = (predictions > threshold).astype(int)\n",
        "\n",
        "# Evaluar el rendimiento del modelo\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(y_test, predicted_classes)\n",
        "report = classification_report(y_test, predicted_classes)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
